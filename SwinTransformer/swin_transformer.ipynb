{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Pytorch Implementation of [SwinTransformer](https://arxiv.org/pdf/2103.14030) with the Most Concise Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Position Bias Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "def get_relative_distances(window_size: int) -> torch.Tensor:\n",
    "    indices = torch.tensor([[i, j] for i in range(window_size) for j in range(window_size)])\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    # shape of distance: [window_size ** 2, window_size ** 2, 2]\n",
    "    return distances\n",
    "\n",
    "window_size = 3\n",
    "relative_distances_1 = get_relative_distances(window_size)\n",
    "print(relative_distances_1.shape)\n",
    "# relative_distances_2 = get_relative_distances(window_size) + window_size - 1\n",
    "# pos_embedding = torch.randn(2 * window_size - 1, 2 * window_size - 1)\n",
    "# print(pos_embedding[relative_distances_1[:, :, 0], relative_distances_1[:, :, 1]] == pos_embedding.roll((window_size - 1, window_size - 1), dims=(0, 1))[relative_distances_2[:, :, 0], relative_distances_2[:, :, 1]], \n",
    "#       '\\n', relative_distances_2[:, :, 0], '\\n', relative_distances_2[:, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positionwise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 hidden_features: int,\n",
    "                 out_features: int,\n",
    "                 dropout: float=0.1) -> None:\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, hidden_features)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(hidden_features, out_features)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout1(self.gelu(self.linear1(x)))\n",
    "        return self.dropout2(self.linear2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement: int):\n",
    "        super(CyclicShift, self).__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, H, W, C]\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 window_size: int,\n",
    "                 number_of_heads: int,\n",
    "                 shift_size: int,\n",
    "                 dropout_attention: float = 0.1,\n",
    "                 dropout_proj: float = 0.1) -> None:\n",
    "        super(WindowAttention, self).__init__()\n",
    "        assert in_features % number_of_heads == 0, 'The num of input features (in_features) must be divisible by the number of heads'\n",
    "        self.in_features = in_features\n",
    "        self.window_size = window_size\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.shift_size = shift_size\n",
    "        self.d_k = in_features // number_of_heads\n",
    "        \n",
    "        if self.shift_size:\n",
    "            self.cyclic_shift = CyclicShift(-shift_size)\n",
    "            self.cyclic_back_shift = CyclicShift(shift_size)\n",
    "        \n",
    "        self.linears = nn.ModuleList([nn.Linear(in_features, in_features) for _ in range(4)])\n",
    "        \n",
    "        self.__register_relative_distances(window_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(number_of_heads, 1, 2 * window_size - 1, 2 * window_size - 1))\n",
    "        \n",
    "        self.dropout_attention = nn.Dropout(dropout_attention)\n",
    "        self.dropout_proj = nn.Dropout(dropout_proj)\n",
    "\n",
    "    def __register_relative_distances(self, window_size: int) -> torch.Tensor:\n",
    "        indices = torch.tensor([[i, j] for i in range(window_size) for j in range(window_size)])\n",
    "        distances = indices[None, :, :] - indices[:, None, :]\n",
    "        # shape of distance: [window_size ** 2, window_size ** 2, 2]\n",
    "        self.register_buffer(\"relative_indices\", distances)\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
    "        # shape of input: [B, H, W, C]\n",
    "        if self.shift_size:\n",
    "            x = self.cyclic_shift(x)\n",
    "        b, h, w, _,  = x.shape\n",
    "        nw_h, nw_w = h // self.window_size, w // self.window_size\n",
    "        # shape of query, key, value: [b, h, nw_h, nw_w, window_size, window_size, d_k]\n",
    "        query, key, value = [lin(x).view(b, nw_h, self.window_size, nw_w, self.window_size, self.number_of_heads, self.d_k).permute(0, 5, 1, 3, 2, 4, 6)\n",
    "                             .contiguous().view(b, self.number_of_heads, nw_h * nw_w, self.window_size * self.window_size, self.d_k)\n",
    "                        for lin, x in zip(self.linears, (x, x, x))]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k) + self.pos_embedding[:, :, self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        \n",
    "        if self.shift_size:\n",
    "            scores[:, :, -nw_w:].masked_fill_(mask[0] == 0, -1e9)\n",
    "            scores[:, :, nw_w - 1::nw_w].masked_fill_(mask[1] == 0, -1e9)\n",
    "        p_attn = self.dropout_attention(F.softmax(scores, dim=-1))\n",
    "        # shape of x: [b, h, nw_h, nw_w, window_size, window_size, d_k]\n",
    "        x = torch.matmul(p_attn, value)\n",
    "\n",
    "        x = x.view(b, self.number_of_heads, nw_h, nw_w, self.window_size, self.window_size, self.d_k).permute(0, 2, 4, 3, 5, 1, 6).contiguous().view(b, h, w, -1)\n",
    "        x = self.dropout_proj(self.linears[-1](x))\n",
    "        if self.shift_size:\n",
    "            x = self.cyclic_back_shift(x)\n",
    "        # shape of output: [B, H, W, C]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Block & Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self,\n",
    "                 drop_prob: float = 0.,\n",
    "                 scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "        \n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "        if keep_prob > 0.0 and self.scale_by_keep:\n",
    "            random_tensor.div_(keep_prob)\n",
    "        return x * random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 window_size: int,\n",
    "                 number_of_heads: int,\n",
    "                 shift_size: int = 0,\n",
    "                 ffn_feature_ratio: int = 4,\n",
    "                 dropout_attention: float = 0.1,\n",
    "                 dropout: float = 0.1,\n",
    "                 dropout_path: float = 0.) -> None:\n",
    "        super(SwinBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.attention = WindowAttention(in_features, window_size, number_of_heads, shift_size, dropout_attention, dropout)\n",
    "        self.drop_path = DropPath(dropout_path)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "        self.feed_forward = FeedForward(in_features, in_features * ffn_feature_ratio, in_features, dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
    "        # shape of input: [B, H, W, C]\n",
    "        x = x + self.drop_path(self.attention(self.norm1(x), mask))\n",
    "        return x + self.drop_path(self.feed_forward(self.norm2(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, downscaling_factor: int):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, C, H, W]\n",
    "        b, c, h, w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        # shape of output: [B, H // scale_factor, W // scale_factor, C']\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StageModule(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 in_features: int,\n",
    "                 window_size: int,\n",
    "                 number_of_heads: int, \n",
    "                 ffn_feature_ratio: int,\n",
    "                 layers: int,\n",
    "                 downscaling_factor: int,\n",
    "                 dropout_attention: float=0.1,\n",
    "                 dropout: float=0.1,\n",
    "                 dropout_path: float = 0.):\n",
    "        super(StageModule, self).__init__()\n",
    "        assert layers % 2 == 0, 'number of layers must be divisible by 2 for regular and shifted block'\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.patch_merging = PatchMerging(in_channels, in_features, downscaling_factor)\n",
    "        self.layers_regular = nn.ModuleList([SwinBlock(in_features, window_size, number_of_heads, 0, ffn_feature_ratio, dropout_attention, dropout, dropout_path) for _ in range(layers // 2)])\n",
    "        self.layers_shifted = nn.ModuleList([SwinBlock(in_features, window_size, number_of_heads, window_size // 2, ffn_feature_ratio, dropout_attention, dropout, dropout_path) for _ in range(layers // 2)])\n",
    "        self.__create_mask()\n",
    "        \n",
    "    def __create_mask(self) -> None:\n",
    "        # create left_right_mask\n",
    "        left_right_mask: torch.Tensor = torch.ones(self.window_size, self.window_size, self.window_size, self.window_size, dtype=torch.uint8)\n",
    "        left_right_mask[:, -self.shift_size:, :, :-self.shift_size] = 0\n",
    "        left_right_mask[:, :-self.shift_size, :, -self.shift_size:] = 0\n",
    "        left_right_mask = left_right_mask.view(self.window_size ** 2, self.window_size ** 2)\n",
    "        # create upper_lower_mask\n",
    "        upper_lower_mask: torch.Tensor = torch.ones(self.window_size ** 2, self.window_size ** 2, dtype=torch.uint8)\n",
    "        upper_lower_mask[-self.shift_size * self.window_size:, :-self.shift_size * self.window_size] = 0\n",
    "        upper_lower_mask[:-self.shift_size * self.window_size, -self.shift_size * self.window_size:] = 0\n",
    "        self.register_buffer('left_right_mask', left_right_mask)\n",
    "        self.register_buffer('upper_lower_mask', upper_lower_mask)\n",
    "\n",
    "    def forward(self, \n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, C, H, W]\n",
    "        x = self.patch_merging(x)\n",
    "        for layer_regular, layer_shifted in zip(self.layers_regular, self.layers_shifted):\n",
    "            x = layer_regular(x)\n",
    "            x = layer_shifted(x, (self.upper_lower_mask, self.left_right_mask))\n",
    "        # shape of output: [B, C', H', W']\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 in_channels: int = 3,\n",
    "                 num_classes: int=10,\n",
    "                 in_features: int = 96,\n",
    "                 window_size: int=7,\n",
    "                 heads: Tuple[int, int, int, int] = (3, 6, 12, 24),\n",
    "                 ffn_feature_ratio: int = 4,\n",
    "                 num_layers: Tuple[int, int, int, int] = (2, 2, 6, 2),\n",
    "                 downscaling_factors: Tuple[int, int, int, int] = (4, 2, 2, 2),\n",
    "                 dropout_attention: float=0.1,\n",
    "                 dropout: float=0.1,\n",
    "                 dropout_path: float = 0.) -> None:\n",
    "        super(SwinTransformer, self).__init__()\n",
    "        self.stage1 = StageModule(in_channels, in_features, window_size, heads[0], ffn_feature_ratio, num_layers[0], downscaling_factors[0], dropout_attention, dropout, dropout_path)\n",
    "        self.stage2 = StageModule(in_features, in_features * 2, window_size, heads[1], ffn_feature_ratio, num_layers[1], downscaling_factors[1], dropout_attention, dropout, dropout_path)\n",
    "        self.stage3 = StageModule(in_features * 2, in_features * 4, window_size, heads[2], ffn_feature_ratio, num_layers[2], downscaling_factors[2], dropout_attention, dropout, dropout_path)\n",
    "        self.stage4 = StageModule(in_features * 4, in_features * 8, window_size, heads[3], ffn_feature_ratio, num_layers[3], downscaling_factors[3], dropout_attention, dropout, dropout_path)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(in_features * 8),\n",
    "            nn.Linear(in_features * 8, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, C, H, W]\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        # shape of output: [B, num_classes]\n",
    "        return self.mlp_head(x.mean(dim=[2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_tiny_patch4_window7_224(d_model: int=96, num_layers: tuple[int, int, int, int]=(2, 2, 6, 2), heads: tuple[int, int, int, int]=(3, 6, 12, 24)) -> SwinTransformer:\n",
    "    return SwinTransformer(d_model=d_model, heads=heads, num_layers=num_layers)\n",
    "\n",
    "def swin_small_patch4_window7_224(d_model: int=96, num_layers: tuple[int, int, int, int]=(2, 2, 18, 2), heads: tuple[int, int, int, int]=(3, 6, 12, 24)) -> SwinTransformer:\n",
    "    return SwinTransformer(d_model=d_model, heads=heads, num_layers=num_layers)\n",
    "\n",
    "def swin_base_patch4_window7_224(d_model: int=128, num_layers: tuple[int, int, int, int]=(2, 2, 18, 2), heads: tuple[int, int, int, int]=(4, 8, 16, 32)) -> SwinTransformer:\n",
    "    return SwinTransformer(d_model=d_model, heads=heads, num_layers=num_layers)\n",
    "\n",
    "def swin_large_patch4_window7_224(d_model: int=192, num_layers: tuple[int, int, int, int]=(2, 2, 18, 2), heads: tuple[int, int, int, int]=(6, 12, 24, 48)) -> SwinTransformer:\n",
    "    return SwinTransformer(d_model=d_model, heads=heads, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "img_size = 96\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "data_train = datasets.FashionMNIST(root=\"../data\", train=True, download=True, transform=transform)\n",
    "data_val = datasets.FashionMNIST(root=\"../data\", train=False, download=True, transform=transform)\n",
    "loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(data_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_layers = (2, 2, 2, 2)\n",
    "window_size = 3\n",
    "in_channels = 1\n",
    "model = SwinTransformer(in_channels=1, window_size=window_size, num_layers=num_layers).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.6846, Train Acc: 0.7461, Val Loss: 0.4276, Val Acc: 0.8460\n",
      "Epoch: 002, Train Loss: 0.3647, Train Acc: 0.8651, Val Loss: 0.3558, Val Acc: 0.8685\n",
      "Epoch: 003, Train Loss: 0.3190, Train Acc: 0.8809, Val Loss: 0.3190, Val Acc: 0.8812\n",
      "Epoch: 004, Train Loss: 0.2872, Train Acc: 0.8925, Val Loss: 0.3006, Val Acc: 0.8903\n",
      "Epoch: 005, Train Loss: 0.2650, Train Acc: 0.9002, Val Loss: 0.3036, Val Acc: 0.8844\n",
      "Epoch: 006, Train Loss: 0.2465, Train Acc: 0.9082, Val Loss: 0.2970, Val Acc: 0.8938\n",
      "Epoch: 007, Train Loss: 0.2304, Train Acc: 0.9130, Val Loss: 0.2715, Val Acc: 0.9019\n",
      "Epoch: 008, Train Loss: 0.2135, Train Acc: 0.9191, Val Loss: 0.2783, Val Acc: 0.9008\n",
      "Epoch: 009, Train Loss: 0.1996, Train Acc: 0.9242, Val Loss: 0.2684, Val Acc: 0.9051\n",
      "Epoch: 010, Train Loss: 0.1900, Train Acc: 0.9282, Val Loss: 0.2676, Val Acc: 0.9073\n",
      "Epoch: 011, Train Loss: 0.1737, Train Acc: 0.9346, Val Loss: 0.2753, Val Acc: 0.9091\n",
      "Epoch: 012, Train Loss: 0.1676, Train Acc: 0.9359, Val Loss: 0.2668, Val Acc: 0.9103\n",
      "Epoch: 013, Train Loss: 0.1524, Train Acc: 0.9419, Val Loss: 0.2672, Val Acc: 0.9086\n",
      "Epoch: 014, Train Loss: 0.1434, Train Acc: 0.9450, Val Loss: 0.2602, Val Acc: 0.9112\n",
      "Epoch: 015, Train Loss: 0.1342, Train Acc: 0.9485, Val Loss: 0.2749, Val Acc: 0.9140\n",
      "Epoch: 016, Train Loss: 0.1244, Train Acc: 0.9529, Val Loss: 0.2782, Val Acc: 0.9125\n",
      "Epoch: 017, Train Loss: 0.1140, Train Acc: 0.9572, Val Loss: 0.2859, Val Acc: 0.9106\n",
      "Epoch: 018, Train Loss: 0.1105, Train Acc: 0.9581, Val Loss: 0.2862, Val Acc: 0.9096\n",
      "Epoch: 019, Train Loss: 0.0995, Train Acc: 0.9620, Val Loss: 0.2839, Val Acc: 0.9139\n",
      "Epoch: 020, Train Loss: 0.0944, Train Acc: 0.9637, Val Loss: 0.2945, Val Acc: 0.9123\n",
      "Epoch: 021, Train Loss: 0.0881, Train Acc: 0.9661, Val Loss: 0.2975, Val Acc: 0.9148\n",
      "Epoch: 022, Train Loss: 0.0803, Train Acc: 0.9692, Val Loss: 0.3162, Val Acc: 0.9102\n",
      "Epoch: 023, Train Loss: 0.0791, Train Acc: 0.9707, Val Loss: 0.3294, Val Acc: 0.9098\n",
      "Epoch: 024, Train Loss: 0.0738, Train Acc: 0.9722, Val Loss: 0.3250, Val Acc: 0.9138\n",
      "Epoch: 025, Train Loss: 0.0684, Train Acc: 0.9746, Val Loss: 0.3204, Val Acc: 0.9184\n",
      "Epoch: 026, Train Loss: 0.0631, Train Acc: 0.9759, Val Loss: 0.3285, Val Acc: 0.9141\n",
      "Epoch: 027, Train Loss: 0.0603, Train Acc: 0.9774, Val Loss: 0.3497, Val Acc: 0.9125\n",
      "Epoch: 028, Train Loss: 0.0563, Train Acc: 0.9784, Val Loss: 0.3343, Val Acc: 0.9152\n",
      "Epoch: 029, Train Loss: 0.0540, Train Acc: 0.9803, Val Loss: 0.3491, Val Acc: 0.9145\n",
      "Epoch: 030, Train Loss: 0.0519, Train Acc: 0.9799, Val Loss: 0.3415, Val Acc: 0.9187\n",
      "Epoch: 031, Train Loss: 0.0455, Train Acc: 0.9821, Val Loss: 0.3737, Val Acc: 0.9122\n",
      "Epoch: 032, Train Loss: 0.0480, Train Acc: 0.9821, Val Loss: 0.3589, Val Acc: 0.9145\n",
      "Epoch: 033, Train Loss: 0.0420, Train Acc: 0.9843, Val Loss: 0.3784, Val Acc: 0.9116\n",
      "Epoch: 034, Train Loss: 0.0440, Train Acc: 0.9840, Val Loss: 0.3732, Val Acc: 0.9146\n",
      "Epoch: 035, Train Loss: 0.0432, Train Acc: 0.9840, Val Loss: 0.3720, Val Acc: 0.9134\n",
      "Epoch: 036, Train Loss: 0.0394, Train Acc: 0.9853, Val Loss: 0.3901, Val Acc: 0.9151\n",
      "Epoch: 037, Train Loss: 0.0360, Train Acc: 0.9861, Val Loss: 0.3896, Val Acc: 0.9131\n",
      "Epoch: 038, Train Loss: 0.0377, Train Acc: 0.9857, Val Loss: 0.3678, Val Acc: 0.9161\n",
      "Epoch: 039, Train Loss: 0.0370, Train Acc: 0.9864, Val Loss: 0.3842, Val Acc: 0.9151\n",
      "Epoch: 040, Train Loss: 0.0321, Train Acc: 0.9889, Val Loss: 0.4098, Val Acc: 0.9134\n",
      "Epoch: 041, Train Loss: 0.0344, Train Acc: 0.9873, Val Loss: 0.3808, Val Acc: 0.9187\n",
      "Epoch: 042, Train Loss: 0.0300, Train Acc: 0.9888, Val Loss: 0.4198, Val Acc: 0.9115\n",
      "Epoch: 043, Train Loss: 0.0372, Train Acc: 0.9864, Val Loss: 0.4221, Val Acc: 0.9139\n",
      "Epoch: 044, Train Loss: 0.0292, Train Acc: 0.9892, Val Loss: 0.4199, Val Acc: 0.9164\n",
      "Epoch: 045, Train Loss: 0.0293, Train Acc: 0.9891, Val Loss: 0.4065, Val Acc: 0.9144\n",
      "Epoch: 046, Train Loss: 0.0302, Train Acc: 0.9885, Val Loss: 0.3927, Val Acc: 0.9184\n",
      "Epoch: 047, Train Loss: 0.0306, Train Acc: 0.9888, Val Loss: 0.3954, Val Acc: 0.9162\n",
      "Epoch: 048, Train Loss: 0.0289, Train Acc: 0.9899, Val Loss: 0.4068, Val Acc: 0.9186\n",
      "Epoch: 049, Train Loss: 0.0275, Train Acc: 0.9901, Val Loss: 0.4183, Val Acc: 0.9151\n",
      "Epoch: 050, Train Loss: 0.0237, Train Acc: 0.9915, Val Loss: 0.4265, Val Acc: 0.9155\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 50\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = train_count = train_acc = 0\n",
    "    for i, (x, y) in enumerate(loader_train):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y, reduction=\"sum\")\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        with torch.no_grad():\n",
    "            train_loss += l.item()\n",
    "            train_count += y.size(0)\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "    model.eval()\n",
    "    val_loss = val_count = val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader_val:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y, reduction=\"sum\")\n",
    "            val_loss += l.item()\n",
    "            val_count += y.size(0)\n",
    "            val_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "    print(f\"Epoch: {epoch + 1:03d}, Train Loss: {train_loss / train_count:.4f}, Train Acc: {train_acc / train_count:.4f}, Val Loss: {val_loss / val_count:.4f}, Val Acc: {val_acc / val_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
