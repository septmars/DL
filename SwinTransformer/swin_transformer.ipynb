{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Pytorch Implementation of [Swin Transformer](https://arxiv.org/pdf/2103.14030).\n",
    "\n",
    "Swin Transformer is a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size.\n",
    "\n",
    "Swin Transformer is built by replacing the standard multi-head self attention module in a Transformer block by a module based on shifted windows, with other layers kept the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Position Bias Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "def get_relative_distances(window_size: int) -> torch.Tensor:\n",
    "    indices = torch.tensor([[i, j] for i in range(window_size) for j in range(window_size)])\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    # shape of distance: [window_size ** 2, window_size ** 2, 2]\n",
    "    return distances\n",
    "\n",
    "window_size = 3\n",
    "relative_distances_1 = get_relative_distances(window_size)\n",
    "print(relative_distances_1.shape)\n",
    "# relative_distances_2 = get_relative_distances(window_size) + window_size - 1\n",
    "# pos_embedding = torch.randn(2 * window_size - 1, 2 * window_size - 1)\n",
    "# print(pos_embedding[relative_distances_1[:, :, 0], relative_distances_1[:, :, 1]] == pos_embedding.roll((window_size - 1, window_size - 1), dims=(0, 1))[relative_distances_2[:, :, 0], relative_distances_2[:, :, 1]], \n",
    "#       '\\n', relative_distances_2[:, :, 0], '\\n', relative_distances_2[:, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positionwise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 hidden_features: int,\n",
    "                 out_features: int,\n",
    "                 dropout: float=0.1) -> None:\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, hidden_features)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(hidden_features, out_features)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout1(self.gelu(self.linear1(x)))\n",
    "        return self.dropout2(self.linear2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifted Window Attention\n",
    "\n",
    "For efficient modeling, the Swin Transformer computes self-attention within local windows. The windows are aranged to evenly partition the image in a non-overlapping manner. Supposing each window contains $M \\times M$ patches, the computational complexity of a global MSA module and a window based one on an image of $h \\times w$ patches are:\n",
    "\n",
    "$$\\Omega \\text{(MSA)}=4hwC^2+2(hw)^2C$$\n",
    "\n",
    "$$\\Omega \\text{(W-MSA)}=4hwC^2+2M^2hwC$$\n",
    "\n",
    "where the former is quadratic to patch number $hw$, and the latter is linear when $M$ is fixed.\n",
    "\n",
    "The window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, the Swin Transformer proposes a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks.\n",
    "\n",
    "The CyclicShift and mask in the following code snippet is used for efficient batch computation approach for self-attention in shifted window partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement: int):\n",
    "        super(CyclicShift, self).__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, H, W, C]\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing self-attention, the Swin Transformer includes a relative position bias $B \\in \\mathbb{R}^{M^2 \\times M^2}$ to each head in computing similarity:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{SoftMax}(QK^T/\\sqrt[]{d} + B)V$$\n",
    "\n",
    "where $Q,K,V \\in \\mathbb{R}^{M^2 \\times d}$ are the query, key and value matrices; $d$ is the query/key dimension, and $M^2$ is the number of patches in a window. Since the relative position along each axis lies in the range $[-M+1,M-1]$, the Swin Transformer parameterizes a smaller-sized bias matrix $\\hat{B} \\in \\mathbb{R}^{(2M-1) \\times (2M-1)}$, and values in $B$ are taken from $\\hat{B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 window_size: int,\n",
    "                 number_of_heads: int,\n",
    "                 shift_size: int,\n",
    "                 dropout_attention: float = 0.1,\n",
    "                 dropout_proj: float = 0.1) -> None:\n",
    "        super(WindowAttention, self).__init__()\n",
    "        assert in_features % number_of_heads == 0, 'The num of input features (in_features) must be divisible by the number of heads'\n",
    "        self.in_features = in_features\n",
    "        self.window_size = window_size\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.shift_size = shift_size\n",
    "        self.d_k = in_features // number_of_heads\n",
    "        \n",
    "        if self.shift_size:\n",
    "            self.cyclic_shift = CyclicShift(-shift_size)\n",
    "            self.cyclic_back_shift = CyclicShift(shift_size)\n",
    "        \n",
    "        self.linears = nn.ModuleList([nn.Linear(in_features, in_features) for _ in range(4)])\n",
    "        \n",
    "        self.__register_relative_distances(window_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(number_of_heads, 1, 2 * window_size - 1, 2 * window_size - 1))\n",
    "        \n",
    "        self.dropout_attention = nn.Dropout(dropout_attention)\n",
    "        self.dropout_proj = nn.Dropout(dropout_proj)\n",
    "\n",
    "    def __register_relative_distances(self, window_size: int) -> torch.Tensor:\n",
    "        indices = torch.tensor([[i, j] for i in range(window_size) for j in range(window_size)])\n",
    "        distances = indices[None, :, :] - indices[:, None, :]\n",
    "        # shape of distance: [window_size ** 2, window_size ** 2, 2]\n",
    "        self.register_buffer(\"relative_indices\", distances)\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
    "        # shape of input: [B, H, W, C]\n",
    "        if self.shift_size:\n",
    "            x = self.cyclic_shift(x)\n",
    "        b, h, w, _,  = x.shape\n",
    "        nw_h, nw_w = h // self.window_size, w // self.window_size\n",
    "        # shape of query, key, value: [b, h, nw_h, nw_w, window_size, window_size, d_k]\n",
    "        query, key, value = [lin(x).view(b, nw_h, self.window_size, nw_w, self.window_size, self.number_of_heads, self.d_k).permute(0, 5, 1, 3, 2, 4, 6)\n",
    "                             .contiguous().view(b, self.number_of_heads, nw_h * nw_w, self.window_size * self.window_size, self.d_k)\n",
    "                        for lin, x in zip(self.linears, (x, x, x))]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k) + self.pos_embedding[:, :, self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        \n",
    "        if self.shift_size:\n",
    "            scores[:, :, -nw_w:].masked_fill_(mask[0] == 0, -1e9)\n",
    "            scores[:, :, nw_w - 1::nw_w].masked_fill_(mask[1] == 0, -1e9)\n",
    "        p_attn = self.dropout_attention(F.softmax(scores, dim=-1))\n",
    "        # shape of x: [b, h, nw_h, nw_w, window_size, window_size, d_k]\n",
    "        x = torch.matmul(p_attn, value)\n",
    "\n",
    "        x = x.view(b, self.number_of_heads, nw_h, nw_w, self.window_size, self.window_size, self.d_k).permute(0, 2, 4, 3, 5, 1, 6).contiguous().view(b, h, w, -1)\n",
    "        x = self.dropout_proj(self.linears[-1](x))\n",
    "        if self.shift_size:\n",
    "            x = self.cyclic_back_shift(x)\n",
    "        # shape of output: [B, H, W, C]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Block & Add & Norm\n",
    "\n",
    "[DropPath](https://arxiv.org/pdf/1603.09382), also known as Stochastic Depth, is a regularization technique used in neural networks, particularly in residual networks. It randomly \"drops\" entire residual blocks during training, effectively reducing the network's depth dynamically. This helps prevent overfitting and improves generalization by forcing the model to learn from shallower sub-networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self,\n",
    "                 drop_prob: float = 0.,\n",
    "                 scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "        \n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "        if keep_prob > 0.0 and self.scale_by_keep:\n",
    "            random_tensor.div_(keep_prob)\n",
    "        return x * random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nolinearity in between. A LayerNorm layer is applied before each MSA module and each MLP, and a residual conncetion is applied after each module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 window_size: int,\n",
    "                 number_of_heads: int,\n",
    "                 shift_size: int = 0,\n",
    "                 ffn_feature_ratio: int = 4,\n",
    "                 dropout_attention: float = 0.1,\n",
    "                 dropout: float = 0.1,\n",
    "                 dropout_path: float = 0.) -> None:\n",
    "        super(SwinBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.attention = WindowAttention(in_features, window_size, number_of_heads, shift_size, dropout_attention, dropout)\n",
    "        self.drop_path = DropPath(dropout_path)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "        self.feed_forward = FeedForward(in_features, in_features * ffn_feature_ratio, in_features, dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
    "        # shape of input: [B, H, W, C]\n",
    "        x = x + self.drop_path(self.attention(self.norm1(x), mask))\n",
    "        return x + self.drop_path(self.feed_forward(self.norm2(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Merging\n",
    "\n",
    "The Swin Transformer first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT. Each patch is treated as a token and its feature is set as a concatenation of the raw pixel RGB values. A linear embedding layer is applied on this raw-valued feature to project it to an arbitrary dimension $C$.\n",
    "\n",
    "To produce a hierarchical representation, the number of tokens is reduced by patch merging layers as the network gets deeper. For example, the first patch merging layer concatenates the features of each group of $2 \\times 2$ neighboring patches, and applies a linear layer on the $4C$-dimensional concatenated features. \n",
    "\n",
    "Both the patch splitting module and the patch merging module can be implemented by nn.Unfold and a linear projection (also can be implemented by a single convolution operation like the [PatchEmbedding of ViT](../VisionTransformer/vision_transformer.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, downscaling_factor: int):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, C, H, W]\n",
    "        b, c, h, w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        # shape of output: [B, H // scale_factor, W // scale_factor, C']\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage\n",
    "\n",
    "The patch merging(splitting) module and $N$ Transformer block is denoted as \"Stage\".\n",
    "\n",
    "The mask for shift window attention is determined as soon as the window_size and shift_size are determined, so I put the __create_mask method in the StageModule class to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 in_features: int,\n",
    "                 window_size: int,\n",
    "                 number_of_heads: int, \n",
    "                 ffn_feature_ratio: int,\n",
    "                 layers: int,\n",
    "                 downscaling_factor: int,\n",
    "                 dropout_attention: float=0.1,\n",
    "                 dropout: float=0.1,\n",
    "                 dropout_path: float = 0.):\n",
    "        super(StageModule, self).__init__()\n",
    "        assert layers % 2 == 0, 'number of layers must be divisible by 2 for regular and shifted block'\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.patch_merging = PatchMerging(in_channels, in_features, downscaling_factor)\n",
    "        self.layers_regular = nn.ModuleList([SwinBlock(in_features, window_size, number_of_heads, 0, ffn_feature_ratio, dropout_attention, dropout, dropout_path) for _ in range(layers // 2)])\n",
    "        self.layers_shifted = nn.ModuleList([SwinBlock(in_features, window_size, number_of_heads, window_size // 2, ffn_feature_ratio, dropout_attention, dropout, dropout_path) for _ in range(layers // 2)])\n",
    "        self.__create_mask()\n",
    "        \n",
    "    def __create_mask(self) -> None:\n",
    "        # create left_right_mask\n",
    "        left_right_mask: torch.Tensor = torch.ones(self.window_size, self.window_size, self.window_size, self.window_size, dtype=torch.uint8)\n",
    "        left_right_mask[:, -self.shift_size:, :, :-self.shift_size] = 0\n",
    "        left_right_mask[:, :-self.shift_size, :, -self.shift_size:] = 0\n",
    "        left_right_mask = left_right_mask.view(self.window_size ** 2, self.window_size ** 2)\n",
    "        # create upper_lower_mask\n",
    "        upper_lower_mask: torch.Tensor = torch.ones(self.window_size ** 2, self.window_size ** 2, dtype=torch.uint8)\n",
    "        upper_lower_mask[-self.shift_size * self.window_size:, :-self.shift_size * self.window_size] = 0\n",
    "        upper_lower_mask[:-self.shift_size * self.window_size, -self.shift_size * self.window_size:] = 0\n",
    "        self.register_buffer('left_right_mask', left_right_mask)\n",
    "        self.register_buffer('upper_lower_mask', upper_lower_mask)\n",
    "\n",
    "    def forward(self, \n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, C, H, W]\n",
    "        x = self.patch_merging(x)\n",
    "        for layer_regular, layer_shifted in zip(self.layers_regular, self.layers_shifted):\n",
    "            x = layer_regular(x)\n",
    "            x = layer_shifted(x, (self.upper_lower_mask, self.left_right_mask))\n",
    "        # shape of output: [B, C', H', W']\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Transformer\n",
    "\n",
    "The Swin Transformer consists of four stages and a MLP head for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 in_channels: int = 3,\n",
    "                 num_classes: int=10,\n",
    "                 in_features: int = 96,\n",
    "                 window_size: int=7,\n",
    "                 heads: Tuple[int, int, int, int] = (3, 6, 12, 24),\n",
    "                 ffn_feature_ratio: int = 4,\n",
    "                 num_layers: Tuple[int, int, int, int] = (2, 2, 6, 2),\n",
    "                 downscaling_factors: Tuple[int, int, int, int] = (4, 2, 2, 2),\n",
    "                 dropout_attention: float=0.1,\n",
    "                 dropout: float=0.1,\n",
    "                 dropout_path: float = 0.) -> None:\n",
    "        super(SwinTransformer, self).__init__()\n",
    "        self.stage1 = StageModule(in_channels, in_features, window_size, heads[0], ffn_feature_ratio, num_layers[0], downscaling_factors[0], dropout_attention, dropout, dropout_path)\n",
    "        self.stage2 = StageModule(in_features, in_features * 2, window_size, heads[1], ffn_feature_ratio, num_layers[1], downscaling_factors[1], dropout_attention, dropout, dropout_path)\n",
    "        self.stage3 = StageModule(in_features * 2, in_features * 4, window_size, heads[2], ffn_feature_ratio, num_layers[2], downscaling_factors[2], dropout_attention, dropout, dropout_path)\n",
    "        self.stage4 = StageModule(in_features * 4, in_features * 8, window_size, heads[3], ffn_feature_ratio, num_layers[3], downscaling_factors[3], dropout_attention, dropout, dropout_path)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(in_features * 8),\n",
    "            nn.Linear(in_features * 8, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, C, H, W]\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        # shape of output: [B, num_classes]\n",
    "        return self.mlp_head(x.mean(dim=[2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_tiny_patch4_window7_224(in_features: int=96, num_layers: tuple[int, int, int, int]=(2, 2, 6, 2), heads: tuple[int, int, int, int]=(3, 6, 12, 24)) -> SwinTransformer:\n",
    "    return SwinTransformer(in_features=in_features, heads=heads, num_layers=num_layers)\n",
    "\n",
    "def swin_small_patch4_window7_224(in_features: int=96, num_layers: tuple[int, int, int, int]=(2, 2, 18, 2), heads: tuple[int, int, int, int]=(3, 6, 12, 24)) -> SwinTransformer:\n",
    "    return SwinTransformer(in_features=in_features, heads=heads, num_layers=num_layers)\n",
    "\n",
    "def swin_base_patch4_window7_224(in_features: int=128, num_layers: tuple[int, int, int, int]=(2, 2, 18, 2), heads: tuple[int, int, int, int]=(4, 8, 16, 32)) -> SwinTransformer:\n",
    "    return SwinTransformer(in_features=in_features, heads=heads, num_layers=num_layers)\n",
    "\n",
    "def swin_large_patch4_window7_224(in_features: int=192, num_layers: tuple[int, int, int, int]=(2, 2, 18, 2), heads: tuple[int, int, int, int]=(6, 12, 24, 48)) -> SwinTransformer:\n",
    "    return SwinTransformer(in_features=in_features, heads=heads, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "img_size = 96\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "data_train = datasets.FashionMNIST(root=\"../data\", train=True, download=True, transform=transform)\n",
    "data_val = datasets.FashionMNIST(root=\"../data\", train=False, download=True, transform=transform)\n",
    "loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(data_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_layers = (2, 2, 2, 2)\n",
    "window_size = 3\n",
    "in_channels = 1\n",
    "model = SwinTransformer(in_channels=1, window_size=window_size, num_layers=num_layers).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.6846, Train Acc: 0.7461, Val Loss: 0.4276, Val Acc: 0.8460\n",
      "Epoch: 002, Train Loss: 0.3647, Train Acc: 0.8651, Val Loss: 0.3558, Val Acc: 0.8685\n",
      "Epoch: 003, Train Loss: 0.3190, Train Acc: 0.8809, Val Loss: 0.3190, Val Acc: 0.8812\n",
      "Epoch: 004, Train Loss: 0.2872, Train Acc: 0.8925, Val Loss: 0.3006, Val Acc: 0.8903\n",
      "Epoch: 005, Train Loss: 0.2650, Train Acc: 0.9002, Val Loss: 0.3036, Val Acc: 0.8844\n",
      "Epoch: 006, Train Loss: 0.2465, Train Acc: 0.9082, Val Loss: 0.2970, Val Acc: 0.8938\n",
      "Epoch: 007, Train Loss: 0.2304, Train Acc: 0.9130, Val Loss: 0.2715, Val Acc: 0.9019\n",
      "Epoch: 008, Train Loss: 0.2135, Train Acc: 0.9191, Val Loss: 0.2783, Val Acc: 0.9008\n",
      "Epoch: 009, Train Loss: 0.1996, Train Acc: 0.9242, Val Loss: 0.2684, Val Acc: 0.9051\n",
      "Epoch: 010, Train Loss: 0.1900, Train Acc: 0.9282, Val Loss: 0.2676, Val Acc: 0.9073\n",
      "Epoch: 011, Train Loss: 0.1737, Train Acc: 0.9346, Val Loss: 0.2753, Val Acc: 0.9091\n",
      "Epoch: 012, Train Loss: 0.1676, Train Acc: 0.9359, Val Loss: 0.2668, Val Acc: 0.9103\n",
      "Epoch: 013, Train Loss: 0.1524, Train Acc: 0.9419, Val Loss: 0.2672, Val Acc: 0.9086\n",
      "Epoch: 014, Train Loss: 0.1434, Train Acc: 0.9450, Val Loss: 0.2602, Val Acc: 0.9112\n",
      "Epoch: 015, Train Loss: 0.1342, Train Acc: 0.9485, Val Loss: 0.2749, Val Acc: 0.9140\n",
      "Epoch: 016, Train Loss: 0.1244, Train Acc: 0.9529, Val Loss: 0.2782, Val Acc: 0.9125\n",
      "Epoch: 017, Train Loss: 0.1140, Train Acc: 0.9572, Val Loss: 0.2859, Val Acc: 0.9106\n",
      "Epoch: 018, Train Loss: 0.1105, Train Acc: 0.9581, Val Loss: 0.2862, Val Acc: 0.9096\n",
      "Epoch: 019, Train Loss: 0.0995, Train Acc: 0.9620, Val Loss: 0.2839, Val Acc: 0.9139\n",
      "Epoch: 020, Train Loss: 0.0944, Train Acc: 0.9637, Val Loss: 0.2945, Val Acc: 0.9123\n",
      "Epoch: 021, Train Loss: 0.0881, Train Acc: 0.9661, Val Loss: 0.2975, Val Acc: 0.9148\n",
      "Epoch: 022, Train Loss: 0.0803, Train Acc: 0.9692, Val Loss: 0.3162, Val Acc: 0.9102\n",
      "Epoch: 023, Train Loss: 0.0791, Train Acc: 0.9707, Val Loss: 0.3294, Val Acc: 0.9098\n",
      "Epoch: 024, Train Loss: 0.0738, Train Acc: 0.9722, Val Loss: 0.3250, Val Acc: 0.9138\n",
      "Epoch: 025, Train Loss: 0.0684, Train Acc: 0.9746, Val Loss: 0.3204, Val Acc: 0.9184\n",
      "Epoch: 026, Train Loss: 0.0631, Train Acc: 0.9759, Val Loss: 0.3285, Val Acc: 0.9141\n",
      "Epoch: 027, Train Loss: 0.0603, Train Acc: 0.9774, Val Loss: 0.3497, Val Acc: 0.9125\n",
      "Epoch: 028, Train Loss: 0.0563, Train Acc: 0.9784, Val Loss: 0.3343, Val Acc: 0.9152\n",
      "Epoch: 029, Train Loss: 0.0540, Train Acc: 0.9803, Val Loss: 0.3491, Val Acc: 0.9145\n",
      "Epoch: 030, Train Loss: 0.0519, Train Acc: 0.9799, Val Loss: 0.3415, Val Acc: 0.9187\n",
      "Epoch: 031, Train Loss: 0.0455, Train Acc: 0.9821, Val Loss: 0.3737, Val Acc: 0.9122\n",
      "Epoch: 032, Train Loss: 0.0480, Train Acc: 0.9821, Val Loss: 0.3589, Val Acc: 0.9145\n",
      "Epoch: 033, Train Loss: 0.0420, Train Acc: 0.9843, Val Loss: 0.3784, Val Acc: 0.9116\n",
      "Epoch: 034, Train Loss: 0.0440, Train Acc: 0.9840, Val Loss: 0.3732, Val Acc: 0.9146\n",
      "Epoch: 035, Train Loss: 0.0432, Train Acc: 0.9840, Val Loss: 0.3720, Val Acc: 0.9134\n",
      "Epoch: 036, Train Loss: 0.0394, Train Acc: 0.9853, Val Loss: 0.3901, Val Acc: 0.9151\n",
      "Epoch: 037, Train Loss: 0.0360, Train Acc: 0.9861, Val Loss: 0.3896, Val Acc: 0.9131\n",
      "Epoch: 038, Train Loss: 0.0377, Train Acc: 0.9857, Val Loss: 0.3678, Val Acc: 0.9161\n",
      "Epoch: 039, Train Loss: 0.0370, Train Acc: 0.9864, Val Loss: 0.3842, Val Acc: 0.9151\n",
      "Epoch: 040, Train Loss: 0.0321, Train Acc: 0.9889, Val Loss: 0.4098, Val Acc: 0.9134\n",
      "Epoch: 041, Train Loss: 0.0344, Train Acc: 0.9873, Val Loss: 0.3808, Val Acc: 0.9187\n",
      "Epoch: 042, Train Loss: 0.0300, Train Acc: 0.9888, Val Loss: 0.4198, Val Acc: 0.9115\n",
      "Epoch: 043, Train Loss: 0.0372, Train Acc: 0.9864, Val Loss: 0.4221, Val Acc: 0.9139\n",
      "Epoch: 044, Train Loss: 0.0292, Train Acc: 0.9892, Val Loss: 0.4199, Val Acc: 0.9164\n",
      "Epoch: 045, Train Loss: 0.0293, Train Acc: 0.9891, Val Loss: 0.4065, Val Acc: 0.9144\n",
      "Epoch: 046, Train Loss: 0.0302, Train Acc: 0.9885, Val Loss: 0.3927, Val Acc: 0.9184\n",
      "Epoch: 047, Train Loss: 0.0306, Train Acc: 0.9888, Val Loss: 0.3954, Val Acc: 0.9162\n",
      "Epoch: 048, Train Loss: 0.0289, Train Acc: 0.9899, Val Loss: 0.4068, Val Acc: 0.9186\n",
      "Epoch: 049, Train Loss: 0.0275, Train Acc: 0.9901, Val Loss: 0.4183, Val Acc: 0.9151\n",
      "Epoch: 050, Train Loss: 0.0237, Train Acc: 0.9915, Val Loss: 0.4265, Val Acc: 0.9155\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 50\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = train_count = train_acc = 0\n",
    "    for i, (x, y) in enumerate(loader_train):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y, reduction=\"sum\")\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        with torch.no_grad():\n",
    "            train_loss += l.item()\n",
    "            train_count += y.size(0)\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "    model.eval()\n",
    "    val_loss = val_count = val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader_val:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y, reduction=\"sum\")\n",
    "            val_loss += l.item()\n",
    "            val_count += y.size(0)\n",
    "            val_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "    print(f\"Epoch: {epoch + 1:03d}, Train Loss: {train_loss / train_count:.4f}, Train Acc: {train_acc / train_count:.4f}, Val Loss: {val_loss / val_count:.4f}, Val Acc: {val_acc / val_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. https://github.com/berniwal/swin-transformer-pytorch/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
