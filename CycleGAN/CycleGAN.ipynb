{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Pytorch Implementation of [CycleGAN](https://arxiv.org/pdf/1703.10593)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The generator consists of the following modules:\n",
    "1. a $7 \\times 7$ Convolution-InstanceNorm-ReLU layer.\n",
    "2. two $3 \\times 3$ Convolution-InstanceNorm-ReLU layers.\n",
    "3. six resudial blocks that contains two $3 \\times 3$ convolutional layers for $128 \\times 128$ images and nine resudial blocks for $256 \\times 256$ images.\n",
    "4. two $3 \\times 3$ fractional-strided-Convolution-InstanceNorm-ReLU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 n_residual_blocks: int) -> None:\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=3, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *[ResidualBlock(256) for _ in range(n_residual_blocks)],\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, input_channels, kernel_size=7, padding=3, padding_mode='reflect')\n",
    "        )\n",
    "        self.layers.apply(weights_init_normal)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The Discriminator consists of the following modules:\n",
    "1. four $4 \\times 4$ Convolution-InstanceNorm-LeakyReLU layers.\n",
    "2. a convolution to produce a 1-dimensional output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 normalize: bool = True) -> None:\n",
    "        super(DiscriminatorBlock, self).__init__()\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: Tuple[int, int, int]) -> None:\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 16, width // 16)\n",
    "        self.layers = nn.Sequential(\n",
    "            DiscriminatorBlock(channels, 64, normalize=False),\n",
    "            DiscriminatorBlock(64, 128),\n",
    "            DiscriminatorBlock(128, 256),\n",
    "            DiscriminatorBlock(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
    "        )\n",
    "        self.apply(weights_init_normal)\n",
    "\n",
    "    def forward(self,\n",
    "                img: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "I use the first 200 images in the monet2photo dataset for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_name: str,\n",
    "                 transforms_: List[nn.Module],\n",
    "                 mode: str,\n",
    "                 max_examples: int = 200):\n",
    "        DATA_DIR = os.path.join('..', 'data', dataset_name)\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.path_a = os.path.join(DATA_DIR, f'{mode}A')\n",
    "        self.path_b = os.path.join(DATA_DIR, f'{mode}B')\n",
    "        self.files_a = os.listdir(self.path_a)[:max_examples]\n",
    "        self.files_b = os.listdir(self.path_b)[:max_examples]\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    index: int) -> Tuple[torch.Tensor]:\n",
    "        return self.transform(plt.imread(os.path.join(self.path_a, self.files_a[index % len(self.files_a)]))), \\\n",
    "            self.transform(plt.imread(os.path.join(self.path_b, self.files_b[index % len(self.files_b)])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_a), len(self.files_b))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayBuffer\n",
    "\n",
    "To reduce model oscillation, the CycleGAN update the discriminator using a history of generated images rather than produced by the latest generators, and keep a buffer that stores the 50 previously created images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        data = data.detach()\n",
    "        res = []\n",
    "        for ele in data:\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(ele)\n",
    "                res.append(ele)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    res.append(self.data[i].clone())\n",
    "                    self.data[i] = ele\n",
    "                else:\n",
    "                    res.append(ele)\n",
    "        return torch.stack(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "\n",
    "The objective of CycleGAN contains three types of terms: adversarial losses for matching the distribution of generated images to the data distribution in the target domain; and cycle consistency losses to prevent the learned mappings G and F from contradicting each other; and identity loss to encourage the mapping to preserve color composition between the input and the output.\n",
    "\n",
    "1. For adversarial loss, CycleGAN replaces the negative log likelihood loss by a least-squares loss. This loss is more stable during training and generates higher quality results.\n",
    "2. The cycle consistent loss forces the mapping G to be cycle-consistent because adversarial loss alone does not guarantee that an individual input $x$ and output $y$ are paired up in a meaningful way and often leads to the problem of mode collapse.\n",
    "3. Without identity loss, the generator G and F are free to change the tint of input images when there is no need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 200\n",
    "dataset_name = 'monet2photo'\n",
    "batch_size = 1\n",
    "lr = 0.0002\n",
    "adam_betas = 0.5, 0.999\n",
    "decay_start = 100\n",
    "\n",
    "gan_loss = torch.nn.MSELoss()\n",
    "cycle_loss = torch.nn.L1Loss()\n",
    "identity_loss = torch.nn.L1Loss()\n",
    "\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "img_channels = 3\n",
    "input_shape = (img_channels, img_height, img_width)\n",
    "\n",
    "n_residual_blocks = 9\n",
    "cyclic_loss_coefficient = 10.0\n",
    "identity_loss_coefficient = 5.\n",
    "\n",
    "generator_xy = GeneratorResNet(img_channels, n_residual_blocks).to(device)\n",
    "generator_yx = GeneratorResNet(img_channels, n_residual_blocks).to(device)\n",
    "discriminator_x = Discriminator(input_shape).to(device)\n",
    "discriminator_y = Discriminator(input_shape).to(device)\n",
    "\n",
    "generator_opitimizer = torch.optim.Adam(\n",
    "    chain(generator_xy.parameters(), generator_yx.parameters()), lr, adam_betas\n",
    ")\n",
    "discriminator_optimizer = torch.optim.Adam(\n",
    "    chain(discriminator_x.parameters(), discriminator_y.parameters()), lr, adam_betas\n",
    ")\n",
    "\n",
    "decay_epochs = epochs - decay_start\n",
    "generator_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    generator_opitimizer, lambda e: 1.0 - max(0, e - decay_start) / decay_epochs\n",
    ")\n",
    "discriminator_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    discriminator_optimizer, lambda e: 1.0 - max(0, e - decay_start) / decay_epochs\n",
    ")\n",
    "\n",
    "transforms_train = [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(int(img_height * 1.12), transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomCrop((img_height, img_height)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]\n",
    "\n",
    "transforms_val = [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(dataset_name, transforms_train, 'train'),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True    \n",
    ")\n",
    "\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    ImageDataset(dataset_name, transforms_val, 'test'),\n",
    "    batch_size=5,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img: torch.Tensor,\n",
    "               path: str) -> None:\n",
    "    img = img.cpu()\n",
    "    img_min, img_max = img.min(), img.max()\n",
    "    img = (img - img_min) / (img_max - img_min + 1e-5)\n",
    "    img = img.permute(1, 2, 0)\n",
    "    plt.imsave(path, img)\n",
    "    # plt.imshow(img)\n",
    "    # plt.axis('off')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(n: int):\n",
    "    X, Y = next(iter(valid_dataloader))\n",
    "    generator_xy.eval()\n",
    "    generator_yx.eval()\n",
    "    with torch.no_grad():\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        gen_y = generator_xy(X)\n",
    "        gen_x = generator_yx(Y)\n",
    "        \n",
    "        X = make_grid(X, nrow=5, normalize=True)\n",
    "        Y = make_grid(Y, nrow=5, normalize=True)\n",
    "        gen_x = make_grid(gen_x, nrow=5, normalize=True)\n",
    "        gen_y = make_grid(gen_y, nrow=5, normalize=True)\n",
    "\n",
    "        image_grid = torch.cat([X, gen_y, Y, gen_x], 1)\n",
    "    plot_image(image_grid, path=f'{output_dir}/epoch_{n:03d}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]d:\\Environment\\miniconda\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
      "25it [00:25,  1.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m loss_generator \u001b[38;5;241m=\u001b[39m loss_gan \u001b[38;5;241m+\u001b[39m cyclic_loss_coefficient \u001b[38;5;241m*\u001b[39m loss_cycle \u001b[38;5;241m+\u001b[39m identity_loss_coefficient \u001b[38;5;241m*\u001b[39m loss_identity\n\u001b[0;32m     30\u001b[0m generator_opitimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m loss_generator\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     32\u001b[0m generator_opitimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     34\u001b[0m loss_g \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_generator\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\Environment\\miniconda\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32md:\\Environment\\miniconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32md:\\Environment\\miniconda\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen_x_buffer = ReplayBuffer()\n",
    "gen_y_buffer = ReplayBuffer()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_g = loss_d = 0\n",
    "    count_g = count_d = 0\n",
    "    for i, (X, Y) in tqdm(enumerate(dataloader, 1)):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        true_labels = torch.ones(X.size(0), *discriminator_x.output_shape, device=device, requires_grad=False)\n",
    "        false_labels = torch.zeros(X.size(0), *discriminator_x.output_shape, device=device, requires_grad=False)\n",
    "        \n",
    "        # Train the generators\n",
    "        generator_xy.train()\n",
    "        generator_yx.train()\n",
    "        \n",
    "        loss_identity = identity_loss(generator_yx(X), X) \\\n",
    "            + identity_loss(generator_xy(Y), Y)\n",
    "        \n",
    "        gen_y = generator_xy(X)\n",
    "        gen_x = generator_yx(Y)\n",
    "        \n",
    "        loss_gan = gan_loss(discriminator_y(gen_y), true_labels) \\\n",
    "            + gan_loss(discriminator_x(gen_x), true_labels)\n",
    "        \n",
    "        loss_cycle = cycle_loss(generator_yx(gen_y), X) \\\n",
    "            + cycle_loss(generator_xy(gen_x), Y)\n",
    "            \n",
    "        loss_generator = loss_gan + cyclic_loss_coefficient * loss_cycle + identity_loss_coefficient * loss_identity\n",
    "        \n",
    "        generator_opitimizer.zero_grad()\n",
    "        loss_generator.backward()\n",
    "        generator_opitimizer.step()\n",
    "        \n",
    "        loss_g += loss_generator.item()\n",
    "        count_g += X.size(0)\n",
    "        \n",
    "        # Train the discriminators\n",
    "        loss_discriminator = (gan_loss(discriminator_x(X), true_labels) + \n",
    "                              gan_loss(discriminator_x(gen_x_buffer.push_and_pop(gen_x)), false_labels) + \n",
    "                              gan_loss(discriminator_y(Y), true_labels) + \n",
    "                              gan_loss(discriminator_y(gen_y_buffer.push_and_pop(gen_y)), false_labels))\n",
    "        \n",
    "        discriminator_optimizer.zero_grad()\n",
    "        loss_discriminator.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        loss_d += loss_discriminator.item()\n",
    "        count_d += X.size(0)\n",
    "\n",
    "    generator_lr_scheduler.step()\n",
    "    discriminator_lr_scheduler.step()\n",
    "    \n",
    "    print(f'\\nEpoch {epoch:03d}, Generator Loss: {loss_g / count_g:.4f}, Discriminator Loss: {loss_d / count_d:.4f}')\n",
    "    if epoch % 25 == 0:\n",
    "        sample_images(epoch)\n",
    "    # with open(f'{output_dir}/losses.txt', 'a') as f:\n",
    "    #     f.write(f'\\nEpoch {epoch:03d}, Generator Loss: {loss_g / count_g:.4f}, Discriminator Loss: {loss_d / count_d:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator_xy.state_dict(), f'{output_dir}/generator_xy.pth')\n",
    "torch.save(generator_yx.state_dict(), f'{output_dir}/generator_yx.pth')\n",
    "torch.save(discriminator_x.state_dict(), f'{output_dir}/discriminator_x.pth')\n",
    "torch.save(discriminator_y.state_dict(), f'{output_dir}/discriminator_y.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. https://nn.labml.ai/gan/cycle_gan/index.html\n",
    "2. [Official Implementation](https://github.com/junyanz/CycleGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
