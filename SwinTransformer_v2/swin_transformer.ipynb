{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Pytorch Implementation of [SwinTransformer v2](https://arxiv.org/pdf/2111.09883) with the Most Concise Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Position Bias Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "def get_relative_distances(window_size: int) -> torch.Tensor:\n",
    "    indices = torch.tensor([[i, j] for i in range(window_size) for j in range(window_size)])\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    # shape of distance: [window_size ** 2, window_size ** 2, 2]\n",
    "    return distances\n",
    "\n",
    "window_size = 3\n",
    "relative_distances_1 = get_relative_distances(window_size)\n",
    "print(relative_distances_1.shape)\n",
    "# relative_distances_2 = get_relative_distances(window_size) + window_size - 1\n",
    "# pos_embedding = torch.randn(2 * window_size - 1, 2 * window_size - 1)\n",
    "# print(pos_embedding[relative_distances_1[:, :, 0], relative_distances_1[:, :, 1]] == pos_embedding.roll((window_size - 1, window_size - 1), dims=(0, 1))[relative_distances_2[:, :, 0], relative_distances_2[:, :, 1]], \n",
    "#       '\\n', relative_distances_2[:, :, 0], '\\n', relative_distances_2[:, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positionwise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 hidden_features: int,\n",
    "                 out_features: int,\n",
    "                 dropout: float=0.1) -> None:\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, hidden_features)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(hidden_features, out_features)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout1(self.gelu(self.linear1(x)))\n",
    "        return self.dropout2(self.linear2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement: int):\n",
    "        super(CyclicShift, self).__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, H, W, C]\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 window_size: int,\n",
    "                 number_of_heads: int,\n",
    "                 shift_size: int,\n",
    "                 dropout_attention: float = 0.1,\n",
    "                 dropout_proj: float = 0.1,\n",
    "                 meta_network_hidden_features: int = 256,\n",
    "                 sequential_self_attention: bool = False) -> None:\n",
    "        super(WindowAttention, self).__init__()\n",
    "        assert in_features % number_of_heads == 0, 'The num of input features (in_features) must be divisible by the number of heads'\n",
    "        self.in_features = in_features\n",
    "        self.window_size = window_size\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.shift_size = shift_size\n",
    "        self.d_k = in_features // number_of_heads\n",
    "        \n",
    "        if self.shift_size:\n",
    "            self.cyclic_shift = CyclicShift(-shift_size)\n",
    "            self.cyclic_back_shift = CyclicShift(shift_size)\n",
    "        \n",
    "        self.linears = nn.ModuleList([nn.Linear(in_features, in_features) for _ in range(4)])\n",
    "        self.__set_relative_distances()\n",
    "        self.meta_network: nn.Module = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=meta_network_hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=meta_network_hidden_features, out_features=number_of_heads),\n",
    "        )\n",
    "        self.tau = nn.Parameter(torch.ones(1, number_of_heads, 1, 1))\n",
    "        self.dropout_attention = nn.Dropout(dropout_attention)\n",
    "        self.dropout_proj = nn.Dropout(dropout_proj)\n",
    "\n",
    "    def __set_relative_distances(self) -> torch.Tensor:\n",
    "        indices = torch.arange(self.window_size)\n",
    "        coordinates = torch.stack(torch.meshgrid([indices, indices]), dim=0)\n",
    "        coordinates = coordinates.flatten(1).transpose(0, 1)\n",
    "        relative_distances = coordinates[None, :, :] - coordinates[:, None, :]\n",
    "        # shape of relative_distances_log: [window_size ** 2, window_size ** 2, 2]\n",
    "        relative_distances_log = torch.sign(relative_distances) * torch.log(torch.abs(relative_distances) + 1)\n",
    "        self.register_buffer('relative_distances_log', relative_distances_log)\n",
    "    \n",
    "    def __get_positional_encoding(self) -> torch.Tensor:\n",
    "        relative_position_bias: torch.Tensor = self.meta_network(self.relative_distances_log)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous().view(self.num_of_heads, 1, self.window_size ** 2, self.window_size ** 2)\n",
    "        # shape of return: [num_heads, 1, window_size ** 2, window_size ** 2]\n",
    "        return relative_position_bias\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
    "        # shape of input: [B, H, W, C]\n",
    "        if self.shift_size:\n",
    "            x = self.cyclic_shift(x)\n",
    "        b, h, w, _,  = x.shape\n",
    "        nw_h, nw_w = h // self.window_size, w // self.window_size\n",
    "        # shape of query, key, value: [b, h, nw_h, nw_w, window_size, window_size, d_k]\n",
    "        query, key, value = [lin(x).view(b, nw_h, self.window_size, nw_w, self.window_size, self.number_of_heads, self.d_k).permute(0, 5, 1, 3, 2, 4, 6)\n",
    "                             .contiguous().view(b, self.number_of_heads, nw_h * nw_w, self.window_size * self.window_size, self.d_k)\n",
    "                        for lin, x in zip(self.linears, (x, x, x))]\n",
    "        # shape of scores: [b, h, nw_h * nw_w, window_size ** 2, d_k]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.maximum(torch.norm(query, dim=-1, keepdim=True) * torch.norm(key, dim=-1, keepdim=True).transpose(-2, -1), 1e-6)\n",
    "        scores /= max(self.tau, 0.01)\n",
    "        scores += self.__get_positional_encoding()\n",
    "        \n",
    "        if self.shift_size:\n",
    "            scores[:, :, -nw_w:].masked_fill_(mask[0] == 0, -1e9)\n",
    "            scores[:, :, nw_w - 1::nw_w].masked_fill_(mask[1] == 0, -1e9)\n",
    "        p_attn = self.dropout_attention(F.softmax(scores, dim=-1))\n",
    "        # shape of x: [b, h, nw_h, nw_w, window_size, window_size, d_k]\n",
    "        x = torch.matmul(p_attn, value)\n",
    "\n",
    "        x = x.view(b, self.number_of_heads, nw_h, nw_w, self.window_size, self.window_size, self.d_k).permute(0, 2, 4, 3, 5, 1, 6).contiguous().view(b, h, w, -1)\n",
    "        x = self.dropout_proj(self.linears[-1](x))\n",
    "        if self.shift_size:\n",
    "            x = self.cyclic_back_shift(x)\n",
    "        # shape of output: [B, H, W, C]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Block & Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self,\n",
    "                 drop_prob: float = 0.,\n",
    "                 scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "        \n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "        if keep_prob > 0.0 and self.scale_by_keep:\n",
    "            random_tensor.div_(keep_prob)\n",
    "        return x * random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 window_size: int,\n",
    "                 number_of_heads: int,\n",
    "                 shift_size: int = 0,\n",
    "                 ffn_feature_ratio: int = 4,\n",
    "                 dropout_attention: float = 0.1,\n",
    "                 dropout: float = 0.1,\n",
    "                 dropout_path: float = 0.) -> None:\n",
    "        super(SwinBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.attention = WindowAttention(in_features, window_size, number_of_heads, shift_size, dropout_attention, dropout)\n",
    "        self.drop_path = DropPath(dropout_path)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "        self.feed_forward = FeedForward(in_features, in_features * ffn_feature_ratio, in_features, dropout)\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -> torch.Tensor:\n",
    "        # shape of input: [B, H, W, C]\n",
    "        x = x + self.drop_path(self.attention(self.norm1(x), mask))\n",
    "        return x + self.drop_path(self.feed_forward(self.norm2(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, downscaling_factor: int):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, C, H, W]\n",
    "        b, c, h, w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        # shape of output: [B, H // scale_factor, W // scale_factor, C']\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StageModule(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 in_features: int,\n",
    "                 window_size: int,\n",
    "                 number_of_heads: int, \n",
    "                 ffn_feature_ratio: int,\n",
    "                 layers: int,\n",
    "                 downscaling_factor: int,\n",
    "                 dropout_attention: float=0.1,\n",
    "                 dropout: float=0.1,\n",
    "                 dropout_path=0.1) -> None:\n",
    "        super(StageModule, self).__init__()\n",
    "        assert layers % 2 == 0, 'number of layers must be divisible by 2 for regular and shifted block'\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.patch_merging = PatchMerging(in_channels, in_features, downscaling_factor)\n",
    "        self.layers_regular = nn.ModuleList([SwinBlock(in_features, window_size, number_of_heads, 0, ffn_feature_ratio, dropout_attention, dropout, dropout_path) for _ in range(layers // 2)])\n",
    "        self.layers_shifted = nn.ModuleList([SwinBlock(in_features, window_size, number_of_heads, window_size // 2, ffn_feature_ratio, dropout_attention, dropout, dropout_path) for _ in range(layers // 2)])\n",
    "        self.__create_mask()\n",
    "        \n",
    "    def __create_mask(self) -> None:\n",
    "        # create left_right_mask\n",
    "        left_right_mask: torch.Tensor = torch.ones(self.window_size, self.window_size, self.window_size, self.window_size, dtype=torch.uint8)\n",
    "        left_right_mask[:, -self.shift_size:, :, :-self.shift_size] = 0\n",
    "        left_right_mask[:, :-self.shift_size, :, -self.shift_size:] = 0\n",
    "        left_right_mask = left_right_mask.view(self.window_size ** 2, self.window_size ** 2)\n",
    "        # create upper_lower_mask\n",
    "        upper_lower_mask: torch.Tensor = torch.ones(self.window_size ** 2, self.window_size ** 2, dtype=torch.uint8)\n",
    "        upper_lower_mask[-self.shift_size * self.window_size:, :-self.shift_size * self.window_size] = 0\n",
    "        upper_lower_mask[:-self.shift_size * self.window_size, -self.shift_size * self.window_size:] = 0\n",
    "        self.register_buffer('left_right_mask', left_right_mask)\n",
    "        self.register_buffer('upper_lower_mask', upper_lower_mask)\n",
    "\n",
    "    def forward(self, \n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, C, H, W]\n",
    "        x = self.patch_merging(x)\n",
    "        for layer_regular, layer_shifted in zip(self.layers_regular, self.layers_shifted):\n",
    "            x = layer_regular(x)\n",
    "            x = layer_shifted(x, (self.upper_lower_mask, self.left_right_mask))\n",
    "        # shape of output: [B, C', H', W']\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 in_channels: int = 3,\n",
    "                 num_classes: int = 10,\n",
    "                 in_features: int = 96,\n",
    "                 window_size: int = 7,\n",
    "                 heads: Tuple[int, int, int, int] = (3, 6, 12, 24),\n",
    "                 ffn_feature_ratio: int = 4,\n",
    "                 num_layers: Tuple[int, int, int, int] = (2, 2, 6, 2),\n",
    "                 downscaling_factors: Tuple[int, int, int, int] = (4, 2, 2, 2),\n",
    "                 dropout_attention: float = 0.1,\n",
    "                 dropout: float = 0.1,\n",
    "                 dropout_path: float = 0.1) -> None:\n",
    "        super(SwinTransformer, self).__init__()\n",
    "        self.stage1 = StageModule(in_channels, in_features, window_size, heads[0], ffn_feature_ratio, num_layers[0], downscaling_factors[0], dropout_attention, dropout, dropout_path)\n",
    "        self.stage2 = StageModule(in_features, in_features * 2, window_size, heads[1], ffn_feature_ratio, num_layers[1], downscaling_factors[1], dropout_attention, dropout, dropout_path)\n",
    "        self.stage3 = StageModule(in_features * 2, in_features * 4, window_size, heads[2], ffn_feature_ratio, num_layers[2], downscaling_factors[2], dropout_attention, dropout, dropout_path)\n",
    "        self.stage4 = StageModule(in_features * 4, in_features * 8, window_size, heads[3], ffn_feature_ratio, num_layers[3], downscaling_factors[3], dropout_attention, dropout, dropout_path)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(in_features * 8),\n",
    "            nn.Linear(in_features * 8, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of input: [B, C, H, W]\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        # shape of output: [B, num_classes]\n",
    "        return self.mlp_head(x.mean(dim=[2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_tiny_patch4_window7_224(d_model: int=96, num_layers: tuple[int, int, int, int]=(2, 2, 6, 2), heads: tuple[int, int, int, int]=(3, 6, 12, 24)) -> SwinTransformer:\n",
    "    return SwinTransformer(d_model=d_model, heads=heads, num_layers=num_layers)\n",
    "\n",
    "def swin_small_patch4_window7_224(d_model: int=96, num_layers: tuple[int, int, int, int]=(2, 2, 18, 2), heads: tuple[int, int, int, int]=(3, 6, 12, 24)) -> SwinTransformer:\n",
    "    return SwinTransformer(d_model=d_model, heads=heads, num_layers=num_layers)\n",
    "\n",
    "def swin_base_patch4_window7_224(d_model: int=128, num_layers: tuple[int, int, int, int]=(2, 2, 18, 2), heads: tuple[int, int, int, int]=(4, 8, 16, 32)) -> SwinTransformer:\n",
    "    return SwinTransformer(d_model=d_model, heads=heads, num_layers=num_layers)\n",
    "\n",
    "def swin_large_patch4_window7_224(d_model: int=192, num_layers: tuple[int, int, int, int]=(2, 2, 18, 2), heads: tuple[int, int, int, int]=(6, 12, 24, 48)) -> SwinTransformer:\n",
    "    return SwinTransformer(d_model=d_model, heads=heads, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "img_size = 96\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "data_train = datasets.FashionMNIST(root=\"../data\", train=True, download=True, transform=transform)\n",
    "data_val = datasets.FashionMNIST(root=\"../data\", train=False, download=True, transform=transform)\n",
    "loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(data_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_layers = (2, 2, 4, 2)\n",
    "window_size = 3\n",
    "in_channels = 1\n",
    "model = SwinTransformer(in_channels=1, window_size=window_size, num_layers=num_layers).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.6149, Train Acc: 0.7741, Val Loss: 0.4051, Val Acc: 0.8484\n",
      "Epoch: 002, Train Loss: 0.3585, Train Acc: 0.8674, Val Loss: 0.3821, Val Acc: 0.8585\n",
      "Epoch: 003, Train Loss: 0.3142, Train Acc: 0.8823, Val Loss: 0.3202, Val Acc: 0.8794\n",
      "Epoch: 004, Train Loss: 0.2847, Train Acc: 0.8941, Val Loss: 0.3064, Val Acc: 0.8844\n",
      "Epoch: 005, Train Loss: 0.2627, Train Acc: 0.9017, Val Loss: 0.2867, Val Acc: 0.8935\n",
      "Epoch: 006, Train Loss: 0.2417, Train Acc: 0.9082, Val Loss: 0.2700, Val Acc: 0.9024\n",
      "Epoch: 007, Train Loss: 0.2240, Train Acc: 0.9157, Val Loss: 0.2989, Val Acc: 0.8912\n",
      "Epoch: 008, Train Loss: 0.2095, Train Acc: 0.9202, Val Loss: 0.2691, Val Acc: 0.9027\n",
      "Epoch: 009, Train Loss: 0.1950, Train Acc: 0.9261, Val Loss: 0.2655, Val Acc: 0.9051\n",
      "Epoch: 010, Train Loss: 0.1800, Train Acc: 0.9313, Val Loss: 0.2564, Val Acc: 0.9086\n",
      "Epoch: 011, Train Loss: 0.1719, Train Acc: 0.9352, Val Loss: 0.2547, Val Acc: 0.9126\n",
      "Epoch: 012, Train Loss: 0.1564, Train Acc: 0.9399, Val Loss: 0.2556, Val Acc: 0.9134\n",
      "Epoch: 013, Train Loss: 0.1449, Train Acc: 0.9453, Val Loss: 0.2655, Val Acc: 0.9137\n",
      "Epoch: 014, Train Loss: 0.1327, Train Acc: 0.9489, Val Loss: 0.2706, Val Acc: 0.9110\n",
      "Epoch: 015, Train Loss: 0.1250, Train Acc: 0.9526, Val Loss: 0.2935, Val Acc: 0.9103\n",
      "Epoch: 016, Train Loss: 0.1145, Train Acc: 0.9570, Val Loss: 0.2887, Val Acc: 0.9113\n",
      "Epoch: 017, Train Loss: 0.1067, Train Acc: 0.9593, Val Loss: 0.2805, Val Acc: 0.9127\n",
      "Epoch: 018, Train Loss: 0.0982, Train Acc: 0.9631, Val Loss: 0.2967, Val Acc: 0.9131\n",
      "Epoch: 019, Train Loss: 0.0899, Train Acc: 0.9662, Val Loss: 0.2927, Val Acc: 0.9172\n",
      "Epoch: 020, Train Loss: 0.0854, Train Acc: 0.9681, Val Loss: 0.3048, Val Acc: 0.9129\n",
      "Epoch: 021, Train Loss: 0.0810, Train Acc: 0.9691, Val Loss: 0.3055, Val Acc: 0.9167\n",
      "Epoch: 022, Train Loss: 0.0755, Train Acc: 0.9723, Val Loss: 0.3003, Val Acc: 0.9190\n",
      "Epoch: 023, Train Loss: 0.0693, Train Acc: 0.9742, Val Loss: 0.3284, Val Acc: 0.9103\n",
      "Epoch: 024, Train Loss: 0.0668, Train Acc: 0.9744, Val Loss: 0.3224, Val Acc: 0.9116\n",
      "Epoch: 025, Train Loss: 0.0613, Train Acc: 0.9764, Val Loss: 0.3293, Val Acc: 0.9159\n",
      "Epoch: 026, Train Loss: 0.0613, Train Acc: 0.9765, Val Loss: 0.3508, Val Acc: 0.9133\n",
      "Epoch: 027, Train Loss: 0.0506, Train Acc: 0.9804, Val Loss: 0.3582, Val Acc: 0.9116\n",
      "Epoch: 028, Train Loss: 0.0518, Train Acc: 0.9806, Val Loss: 0.3439, Val Acc: 0.9175\n",
      "Epoch: 029, Train Loss: 0.0485, Train Acc: 0.9823, Val Loss: 0.3560, Val Acc: 0.9154\n",
      "Epoch: 030, Train Loss: 0.0480, Train Acc: 0.9819, Val Loss: 0.3729, Val Acc: 0.9125\n",
      "Epoch: 031, Train Loss: 0.0475, Train Acc: 0.9820, Val Loss: 0.3648, Val Acc: 0.9146\n",
      "Epoch: 032, Train Loss: 0.0427, Train Acc: 0.9839, Val Loss: 0.3865, Val Acc: 0.9139\n",
      "Epoch: 033, Train Loss: 0.0422, Train Acc: 0.9845, Val Loss: 0.3981, Val Acc: 0.9108\n",
      "Epoch: 034, Train Loss: 0.0401, Train Acc: 0.9849, Val Loss: 0.3733, Val Acc: 0.9139\n",
      "Epoch: 035, Train Loss: 0.0395, Train Acc: 0.9856, Val Loss: 0.4325, Val Acc: 0.9071\n",
      "Epoch: 036, Train Loss: 0.0376, Train Acc: 0.9860, Val Loss: 0.4004, Val Acc: 0.9122\n",
      "Epoch: 037, Train Loss: 0.0360, Train Acc: 0.9864, Val Loss: 0.4073, Val Acc: 0.9145\n",
      "Epoch: 038, Train Loss: 0.0368, Train Acc: 0.9859, Val Loss: 0.3779, Val Acc: 0.9148\n",
      "Epoch: 039, Train Loss: 0.0326, Train Acc: 0.9879, Val Loss: 0.3920, Val Acc: 0.9156\n",
      "Epoch: 040, Train Loss: 0.0313, Train Acc: 0.9889, Val Loss: 0.4005, Val Acc: 0.9148\n",
      "Epoch: 041, Train Loss: 0.0296, Train Acc: 0.9888, Val Loss: 0.4024, Val Acc: 0.9138\n",
      "Epoch: 042, Train Loss: 0.0303, Train Acc: 0.9896, Val Loss: 0.3945, Val Acc: 0.9145\n",
      "Epoch: 043, Train Loss: 0.0304, Train Acc: 0.9891, Val Loss: 0.4097, Val Acc: 0.9158\n",
      "Epoch: 044, Train Loss: 0.0298, Train Acc: 0.9892, Val Loss: 0.4427, Val Acc: 0.9164\n",
      "Epoch: 045, Train Loss: 0.0269, Train Acc: 0.9902, Val Loss: 0.4204, Val Acc: 0.9160\n",
      "Epoch: 046, Train Loss: 0.0270, Train Acc: 0.9905, Val Loss: 0.4129, Val Acc: 0.9145\n",
      "Epoch: 047, Train Loss: 0.0267, Train Acc: 0.9907, Val Loss: 0.4221, Val Acc: 0.9169\n",
      "Epoch: 048, Train Loss: 0.0261, Train Acc: 0.9907, Val Loss: 0.4223, Val Acc: 0.9149\n",
      "Epoch: 049, Train Loss: 0.0253, Train Acc: 0.9912, Val Loss: 0.4188, Val Acc: 0.9166\n",
      "Epoch: 050, Train Loss: 0.0253, Train Acc: 0.9905, Val Loss: 0.4337, Val Acc: 0.9165\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 50\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = train_count = train_acc = 0\n",
    "    for i, (x, y) in enumerate(loader_train):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y, reduction=\"sum\")\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        with torch.no_grad():\n",
    "            train_loss += l.item()\n",
    "            train_count += y.size(0)\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "    model.eval()\n",
    "    val_loss = val_count = val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader_val:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y, reduction=\"sum\")\n",
    "            val_loss += l.item()\n",
    "            val_count += y.size(0)\n",
    "            val_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "    print(f\"Epoch: {epoch + 1:03d}, Train Loss: {train_loss / train_count:.4f}, Train Acc: {train_acc / train_count:.4f}, Val Loss: {val_loss / val_count:.4f}, Val Acc: {val_acc / val_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
