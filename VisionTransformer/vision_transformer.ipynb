{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Pytorch Implementation of [Vision Transformer](https://arxiv.org/pdf/2010.11929).\n",
    "\n",
    "Vision Transformer (ViT) extracts patches from images and feed them into a Transformer encoder to obtain a global representation, which will finally be transformed for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module: nn.Module, N: int) -> nn.ModuleList:\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchEmbedding\n",
    "\n",
    "The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, ViT reshapes the image $x \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of falttened 2D patches $x_p \\in \\mathbb{R}^{N \\times (P^2 \\cdot C)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N = HW / P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so ViT flattens the patches and map to $D$ dimensions with a trainable linear projection. The output of this projection is called the patch embeddings.\n",
    "\n",
    "Spliting an image into patches and linearly projecting these flattened patches can be simplified as a single convolution operation, where both the kernel size and the stride size are set to the patch size.\n",
    "\n",
    "ViT prepends a learnable embedding (cls_token in the following code snippet) to the sequence of embeded patches, whose state at the output of the Transformer encoder serves as the image representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 patch_size: int = 16,\n",
    "                 channels_in: int = 3,\n",
    "                 d_model: int = 512) -> None:\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        # self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.conv = nn.Conv2d(channels_in, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape of x: [b, c, h, w]\n",
    "        # shape of output: [b, d, h // p, w // p] -> [b, d, n_p] -> [b, n_p, d] -> [b, n_p + 1, d]\n",
    "        x = self.conv(x).flatten(2).transpose(1, 2)\n",
    "        return torch.cat([self.cls_token.expand(x.size(0), -1, -1), x], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position embeddings are added to the patch embeddings to retain positional information. ViT uses standard learnable 1D position embeddings.\n",
    "\n",
    "When feeding images of higher resolution after pre-training, the ViT keeps the patch size the same, which results in a larger effective sequence length. The ViT can handle arbitrary sequence lengths, however, the pre-trained position embeddings may no longer be meaningful. The ViT therefore preform 2D interpolation of the pre-trained position embeddings, according to their location in the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_steps: int,\n",
    "                 d_model: int = 512,\n",
    "                 dropout: float = 0.1) -> None:\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_steps, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.dropout(x + self.pos_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Dot Production Attention and Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size: int) -> torch.Tensor:\n",
    "    attn_shape = (1, size, size)\n",
    "    return torch.triu(torch.ones(attn_shape), 1).type(torch.uint8) == 0\n",
    "\n",
    "def attention(query: torch.Tensor,\n",
    "              key: torch.Tensor,\n",
    "              value: torch.Tensor,\n",
    "              mask: Optional[torch.Tensor] = None,\n",
    "              dropout: Optional[nn.Dropout] = None) -> Tuple[torch.Tensor]:\n",
    "    # shape of query: [b, nq, d_k] or [b, h, nq, d_k]\n",
    "    # shape of key: [b, n, d_k] or [b, h, n, d_k]\n",
    "    # shape of value: [b, n, d_v] or [b, h, n, d_v]\n",
    "    d_k = query.size(-1)\n",
    "    # shape of scores: [b, nq, n] or [b, h, nq, n]\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # shape of mask: [b, 1 or nq, n] or [b, 1, 1 or nq, n]\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    # shape of output: [b, nq, d_v] or [b, h, nq, d_v]\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 h: int,\n",
    "                 d_model: int,\n",
    "                 dropout: float = 0.1) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # add a dimension for Multi-Head attention\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        query, key, value = [lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for lin, x in zip(self.linears, (query, key, value))]\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self,\n",
    "                 size: int,\n",
    "                 dropout: float) -> None:\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                sublayer: Callable) -> torch.Tensor:\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positionwise FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 d_ff: int,\n",
    "                 dropout: float=0.1) -> None:\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w_2(self.dropout(F.gelu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 size: int,\n",
    "                 self_attn: MultiHeadAttention,\n",
    "                 feed_forward: PositionwiseFeedForward,\n",
    "                 dropout: float) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 layer: EncoderLayer,\n",
    "                 N: int) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 num_classes: int) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: Encoder,\n",
    "                 embed,\n",
    "                 generator: Generator) -> None:\n",
    "        super(ViT, self).__init__()\n",
    "        self.embed = embed\n",
    "        self.encoder = encoder\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.generator(self.encode(x)[:, 0])\n",
    "\n",
    "    def encode(self,\n",
    "               x: torch.Tensor) -> torch.Tensor:\n",
    "        # output shape: [b, n + 1, d]\n",
    "        return self.encoder(self.embed(x), mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(img_size: int=96,\n",
    "               patch_size: int=16,\n",
    "               num_channels: int=3,\n",
    "               d_model: int=512,\n",
    "               num_classes: int=10,\n",
    "               h: int=8,\n",
    "               N: int=6,\n",
    "               d_ff: int=2048,\n",
    "               dropout: float=0.1) -> ViT:\n",
    "    attn = MultiHeadAttention(h, d_model, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    p = PositionalEmbedding((img_size // patch_size) ** 2 + 1, d_model, dropout)\n",
    "    model = ViT(Encoder(EncoderLayer(d_model, attn, ff, dropout), N),\n",
    "                nn.Sequential(PatchEmbedding(patch_size, num_channels, d_model), p),\n",
    "                Generator(d_model, num_classes))\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "img_size = 96\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "data_train = datasets.FashionMNIST(root=\"../data\", train=True, download=True, transform=transform)\n",
    "data_val = datasets.FashionMNIST(root=\"../data\", train=False, download=True, transform=transform)\n",
    "loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(data_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "patch_size = 16\n",
    "d_model, d_ff, h, N = 512, 2048, 8, 2\n",
    "dropout = 0.1\n",
    "num_classes = 10\n",
    "model = make_model(img_size, patch_size, 1, d_model, num_classes, h, N, d_ff, dropout).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.0715, Train Acc: 0.9727, Val Loss: 0.5317, Val Acc: 0.8864\n",
      "Epoch: 002, Train Loss: 0.0698, Train Acc: 0.9737, Val Loss: 0.4823, Val Acc: 0.8867\n",
      "Epoch: 003, Train Loss: 0.0691, Train Acc: 0.9733, Val Loss: 0.4955, Val Acc: 0.8841\n",
      "Epoch: 004, Train Loss: 0.0655, Train Acc: 0.9756, Val Loss: 0.5458, Val Acc: 0.8887\n",
      "Epoch: 005, Train Loss: 0.0653, Train Acc: 0.9755, Val Loss: 0.5375, Val Acc: 0.8890\n",
      "Epoch: 006, Train Loss: 0.0616, Train Acc: 0.9776, Val Loss: 0.5268, Val Acc: 0.8831\n",
      "Epoch: 007, Train Loss: 0.0592, Train Acc: 0.9784, Val Loss: 0.5192, Val Acc: 0.8875\n",
      "Epoch: 008, Train Loss: 0.0547, Train Acc: 0.9797, Val Loss: 0.5741, Val Acc: 0.8850\n",
      "Epoch: 009, Train Loss: 0.0536, Train Acc: 0.9799, Val Loss: 0.5692, Val Acc: 0.8895\n",
      "Epoch: 010, Train Loss: 0.0528, Train Acc: 0.9803, Val Loss: 0.5776, Val Acc: 0.8864\n",
      "Epoch: 011, Train Loss: 0.0566, Train Acc: 0.9793, Val Loss: 0.5722, Val Acc: 0.8854\n",
      "Epoch: 012, Train Loss: 0.0529, Train Acc: 0.9800, Val Loss: 0.5601, Val Acc: 0.8897\n",
      "Epoch: 013, Train Loss: 0.0481, Train Acc: 0.9823, Val Loss: 0.5755, Val Acc: 0.8872\n",
      "Epoch: 014, Train Loss: 0.0478, Train Acc: 0.9821, Val Loss: 0.5603, Val Acc: 0.8922\n",
      "Epoch: 015, Train Loss: 0.0511, Train Acc: 0.9813, Val Loss: 0.5622, Val Acc: 0.8900\n",
      "Epoch: 016, Train Loss: 0.0430, Train Acc: 0.9842, Val Loss: 0.5736, Val Acc: 0.8874\n",
      "Epoch: 017, Train Loss: 0.0467, Train Acc: 0.9830, Val Loss: 0.5509, Val Acc: 0.8950\n",
      "Epoch: 018, Train Loss: 0.0431, Train Acc: 0.9846, Val Loss: 0.5778, Val Acc: 0.8871\n",
      "Epoch: 019, Train Loss: 0.0451, Train Acc: 0.9841, Val Loss: 0.5797, Val Acc: 0.8885\n",
      "Epoch: 020, Train Loss: 0.0422, Train Acc: 0.9849, Val Loss: 0.5757, Val Acc: 0.8880\n",
      "Epoch: 021, Train Loss: 0.0407, Train Acc: 0.9859, Val Loss: 0.6118, Val Acc: 0.8911\n",
      "Epoch: 022, Train Loss: 0.0419, Train Acc: 0.9848, Val Loss: 0.6265, Val Acc: 0.8870\n",
      "Epoch: 023, Train Loss: 0.0405, Train Acc: 0.9850, Val Loss: 0.5913, Val Acc: 0.8872\n",
      "Epoch: 024, Train Loss: 0.0398, Train Acc: 0.9857, Val Loss: 0.6276, Val Acc: 0.8891\n",
      "Epoch: 025, Train Loss: 0.0386, Train Acc: 0.9862, Val Loss: 0.6073, Val Acc: 0.8924\n",
      "Epoch: 026, Train Loss: 0.0383, Train Acc: 0.9862, Val Loss: 0.6227, Val Acc: 0.8879\n",
      "Epoch: 027, Train Loss: 0.0373, Train Acc: 0.9862, Val Loss: 0.5859, Val Acc: 0.8907\n",
      "Epoch: 028, Train Loss: 0.0351, Train Acc: 0.9877, Val Loss: 0.6180, Val Acc: 0.8899\n",
      "Epoch: 029, Train Loss: 0.0355, Train Acc: 0.9875, Val Loss: 0.6258, Val Acc: 0.8892\n",
      "Epoch: 030, Train Loss: 0.0364, Train Acc: 0.9871, Val Loss: 0.6036, Val Acc: 0.8920\n",
      "Epoch: 031, Train Loss: 0.0347, Train Acc: 0.9879, Val Loss: 0.6037, Val Acc: 0.8922\n",
      "Epoch: 032, Train Loss: 0.0321, Train Acc: 0.9882, Val Loss: 0.5992, Val Acc: 0.8901\n",
      "Epoch: 033, Train Loss: 0.0342, Train Acc: 0.9881, Val Loss: 0.6375, Val Acc: 0.8887\n",
      "Epoch: 034, Train Loss: 0.0331, Train Acc: 0.9883, Val Loss: 0.6213, Val Acc: 0.8867\n",
      "Epoch: 035, Train Loss: 0.0332, Train Acc: 0.9879, Val Loss: 0.6212, Val Acc: 0.8920\n",
      "Epoch: 036, Train Loss: 0.0297, Train Acc: 0.9897, Val Loss: 0.6565, Val Acc: 0.8928\n",
      "Epoch: 037, Train Loss: 0.0315, Train Acc: 0.9893, Val Loss: 0.6444, Val Acc: 0.8844\n",
      "Epoch: 038, Train Loss: 0.0326, Train Acc: 0.9880, Val Loss: 0.6482, Val Acc: 0.8900\n",
      "Epoch: 039, Train Loss: 0.0296, Train Acc: 0.9895, Val Loss: 0.6791, Val Acc: 0.8860\n",
      "Epoch: 040, Train Loss: 0.0325, Train Acc: 0.9890, Val Loss: 0.6393, Val Acc: 0.8858\n",
      "Epoch: 041, Train Loss: 0.0288, Train Acc: 0.9898, Val Loss: 0.6064, Val Acc: 0.8939\n",
      "Epoch: 042, Train Loss: 0.0247, Train Acc: 0.9914, Val Loss: 0.6990, Val Acc: 0.8896\n",
      "Epoch: 043, Train Loss: 0.0299, Train Acc: 0.9897, Val Loss: 0.6185, Val Acc: 0.8889\n",
      "Epoch: 044, Train Loss: 0.0296, Train Acc: 0.9893, Val Loss: 0.6211, Val Acc: 0.8917\n",
      "Epoch: 045, Train Loss: 0.0257, Train Acc: 0.9909, Val Loss: 0.6491, Val Acc: 0.8906\n",
      "Epoch: 046, Train Loss: 0.0302, Train Acc: 0.9893, Val Loss: 0.6427, Val Acc: 0.8920\n",
      "Epoch: 047, Train Loss: 0.0264, Train Acc: 0.9905, Val Loss: 0.6394, Val Acc: 0.8917\n",
      "Epoch: 048, Train Loss: 0.0267, Train Acc: 0.9906, Val Loss: 0.6581, Val Acc: 0.8914\n",
      "Epoch: 049, Train Loss: 0.0275, Train Acc: 0.9902, Val Loss: 0.6146, Val Acc: 0.8885\n",
      "Epoch: 050, Train Loss: 0.0248, Train Acc: 0.9911, Val Loss: 0.6627, Val Acc: 0.8942\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 50\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = train_count = train_acc = 0\n",
    "    for i, (x, y) in enumerate(loader_train):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y, reduction=\"sum\")\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        with torch.no_grad():\n",
    "            train_loss += l.item()\n",
    "            train_count += y.size(0)\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "    model.eval()\n",
    "    val_loss = val_count = val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader_val:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y, reduction=\"sum\")\n",
    "            val_loss += l.item()\n",
    "            val_count += y.size(0)\n",
    "            val_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "    print(f\"Epoch: {epoch + 1:03d}, Train Loss: {train_loss / train_count:.4f}, Train Acc: {train_acc / train_count:.4f}, Val Loss: {val_loss / val_count:.4f}, Val Acc: {val_acc / val_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. [Official implementation](https://github.com/google-research/vision_transformer)\n",
    "2. [d2l](https://d2l.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
